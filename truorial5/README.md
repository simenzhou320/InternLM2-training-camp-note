# LMDeployéƒ¨ç½²LLMå®æˆ˜

## å¼€å‘æœºé…ç½®
ç”±äºLMDeployæ˜¯åŸºäºCUDA12å¼€å‘çš„ï¼Œæ‰€ä»¥ç¯å¢ƒé…ç½®éœ€è¦æ³¨æ„é€‰æ‹©CUDA12ã€‚
é€‰æ‹©é…ç½®è¿‡ç¨‹ç•¥ã€‚ã€‚ã€‚

## é…ç½®ç¯å¢ƒ
InternStudioä¸Šæä¾›äº†å¿«é€Ÿåˆ›å»ºcondaç¯å¢ƒçš„æ–¹æ³•ã€‚æ‰“å¼€å‘½ä»¤è¡Œç»ˆç«¯ï¼Œåˆ›å»ºä¸€ä¸ªåä¸ºlmdeployçš„ç¯å¢ƒï¼š
```shall
studio-conda -t lmdeploy -o pytorch-2.1.2
```
æ¿€æ´»ç¯å¢ƒ
```shall
conda activate lmdeploy
```

å®‰è£…LMDeploy0.3.0
```shall
pip install lmdeploy[all]==0.3.0
```

## ä¸‹è½½æ¨¡å‹å¹¶æµ‹è¯•
ä»shareç›®å½•å»ºç«‹ä¸€ä¸ªè½¯è¿æ¥
```shall
ln -s /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b /root/
```

æµ‹è¯•
```shall
lmdeploy chat /root/internlm2-chat-1_8b
```
é€Ÿåº¦è¦æ¯”ç›´æ¥ä½¿ç”¨Transformerå¿«å¾ˆå¤š

## LMDeployæ¨¡å‹é‡åŒ–(lite)
ä½¿ç”¨KV Cacheè®¾ç½®ç¼“å­˜å¤§å°
```shall
lmdeploy chat /root/internlm2-chat-1_8b --cache-max-entry-count 0.5
```
é»˜è®¤æ˜¯0.8ï¼Œè®¾ç½®0.5åï¼Œæ˜¾å­˜å ç”¨æ˜æ˜¾å°äº†ä¸€äº›

## ä½¿ç”¨W4A16é‡åŒ–
å®‰è£…ä¾èµ–åº“
```shall
pip install einops==0.7.0
```

æ‰§è¡Œæ¨¡å‹é‡åŒ–
```shall
lmdeploy lite auto_awq \
   /root/internlm2-chat-1_8b \
  --calib-dataset 'ptb' \
  --calib-samples 128 \
  --calib-seqlen 1024 \
  --w-bits 4 \
  --w-group-size 128 \
  --work-dir /root/internlm2-chat-1_8b-4bit
```
![image](images/tuorial5_1.png)
ç»è¿‡ä¸€æ®µæ—¶é—´çš„ç­‰å¾…ï¼Œé‡åŒ–å®Œæˆ

## æµ‹è¯•é‡åŒ–åçš„æ¨¡å‹æ•ˆæœ
è®¾ç½®KV Cacheæœ€å¤§å ç”¨æ¯”ä¾‹ä¸º0.4ï¼Œå¼€å¯W4A16é‡åŒ–ï¼Œä»¥å‘½ä»¤è¡Œæ–¹å¼ä¸æ¨¡å‹å¯¹è¯
```shall
lmdeploy chat /root/models/internlm2-chat-1_8b-4bit --model-format awq --cache-max-entry-count 0.4
```
![image](images/tuorial5_2.png)

## å¯åŠ¨APIæœåŠ¡å™¨,ä½¿ç”¨é‡åŒ–åçš„æ¨¡å‹ï¼Œä¸”è°ƒæ•´KV Cacheä¸º40%
å…ˆæŸ¥çœ‹serverçš„ä½¿ç”¨æ–¹å¼
```shall
lmdeploy serve api_server --help
```
![image](images/tuorial5_3.png)
ç”±å¸®åŠ©ä¿¡æ¯å¾—çŸ¥ï¼Œéœ€è¦è®¾ç½®æ¨¡å‹æ ¼å¼ --model-format awqï¼Œ--cache-max-entry-count 0.40

```shall
lmdeploy serve api_server /root/models/internlm2-chat-1_8b-4bit \
    --model-format awq \
    --cache-max-entry-count 0.40 \
    --quant-policy 0 \
    --server-name 0.0.0.0 \
    --server-port 23333 \
    --tp 1
```
![image](images/tuorial5_4.png)

## æµ‹è¯•APIæœåŠ¡å™¨
å‘½ä»¤è¡Œæ–¹å¼
```shall
lmdeploy serve api_client http://localhost:23333
```
![image](images/tuorial5_5.png)

## ä¸Šå¼ºåº¦ï¼APIæ–¹å¼å¯åŠ¨ï¼Œå¹¶é™„åŠ 2ä¸ªLora
å…ˆæµ‹è¯•æ— é‡åŒ–çš„ç‰ˆæœ¬ï¼Œç”±äºéœ€è¦åŠ å…¥ä¸¤ä¸ªLoraï¼Œè®¾ç½®KV cacheä¸º0.4
ä½¿ç”¨ä¸Šä¸€è¯¾ä¸­å¾®è°ƒçš„ä¸¤ä¸ªLoraï¼Œå•ä¸ªLoraä¸éœ€è¦è®¾ç½®åç§°ï¼Œå¤šä¸ªéœ€è¦ç”¨x=yçš„æ–¹å¼æ¥å¡«å†™
```shall
lmdeploy serve api_server /root/internlm2-chat-1_8b \
    --cache-max-entry-count 0.40 \
    --quant-policy 0 \
    --server-name 0.0.0.0 \
    --server-port 23333 \
    --tp 1 \
    --backend pytorch \ 
    --adapters yykx=/root/models/yykx hdnj=/root/models/hdnj
```
![image](images/tuorial5_8.png)
æŠ¥é”™ï¼Œæç¤ºLoraé…ç½®æ–‡ä»¶ä¸­æœ‰æ— æ³•è¯†åˆ«çš„å‚æ•° layer_replicationï¼Œè¿›å…¥Loraæ–‡ä»¶å¤¹åï¼Œåœ¨adapter_config.jsonä¸­åˆ é™¤ç›¸åº”çš„å‚æ•°åï¼Œå†æ¬¡å¯åŠ¨

![image](images/tuorial5_6.png)
adapters éªŒè¯æˆåŠŸï¼Œä½†æ˜¯ç”±äºæ˜¯pytorchçš„æ–¹å¼å¯åŠ¨ï¼Œçˆ†å†…å­˜äº†ï¼Œå…³æœºå‡çº§é…ç½®åä¾æ—§æç¤ºå†…å­˜ä¸å¤Ÿ

åæ¢äº†vllméƒ¨ç½²ï¼Œå¹¶é™„åŠ Loraåï¼Œå¾—åˆ°æ¸…æ™°çš„æç¤ºï¼šç›®å‰internlmæ¨¡å‹ä¸æ”¯æŒé™„åŠ loraçš„åŠŸèƒ½ã€‚

## æœ¬åœ°éƒ¨ç½²ï¼Œä½¿ç”¨streamlit web ui æ–¹å¼æµ‹è¯•
```python
import streamlit as st
from openai import OpenAI

with st.sidebar:
    model_name = st.sidebar.selectbox(
        'é€‰æ‹©ä¸€ä¸ªé€‰é¡¹',
        ('internlm2', 'yykx', 'hdnj') #åŸæ¥æ˜¯ç”¨äºåˆ‡æ¢Loraçš„ï¼Œç°åœ¨ä¹Ÿæ²¡æ³•ç”¨äº†ï¼Œç•™ä½œä»¥ååŠŸèƒ½å®Œå–„åå†æµ‹è¯•
    )

st.title("ğŸ’¬ Chatbot")

if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "How can I help you?"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt := st.chat_input():
    #client = OpenAI(api_key='internlm2-chat-1_8b')
    client = OpenAI(
        api_key='YOUR_API_KEY',
        base_url="http://0.0.0.0:23333/v1"
    )
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    response = client.chat.completions.create(
        model=model_name,
        messages=st.session_state.messages)
    msg = response.choices[0].message.content
    st.session_state.messages.append({"role": "assistant", "content": msg})
    st.chat_message("assistant").write(msg)
```
ä¿å­˜ä¸º app.py
## å¯åŠ¨streamlit
æ‰“é€šSSH
```shall
ssh -CNg -L 23333:127.0.0.1:23333 root@ssh.intern-ai.org.cn -p <ä½ çš„sshç«¯å£å·>
```
å¯åŠ¨App
```shall
streamlit run app.py
```
![image](images/tuorial5_7.png)
åŸæ¨¡å‹æµ‹è¯•é€šè¿‡